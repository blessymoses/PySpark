https://archive.eksworkshop.com/advanced/430_emr_on_eks/autoscaling/#dynamic-resource-allocation

https://spark.apache.org/docs/latest/running-on-kubernetes.html

https://jaceklaskowski.github.io/spark-kubernetes-book/configuration-properties/#sparkkubernetesallocationexecutortimeout

https://outshift.cisco.com/blog/spark-k8s-internals



spark.dynamicAllocation.executorIdleTimeout

spark.executor.instances - specifies the initial number of executor pods to launch for a Spark application when running on Kubernetes, relevant when dynamic resource allocation is disabled (spark.dynamicAllocation.enabled=false)

spark.kubernetes.executor.deleteOnTermination - default: True, If you set spark.kubernetes.executor.deleteOnTermination to false, executor pods will remain in the cluster after termination. This can be useful for debugging purposes
- Leaving terminated executor pods in the cluster can consume cluster resources and may lead to clutter over time. It’s advisable to enable this setting (false) temporarily for debugging and revert it back to true once debugging is complete.


"spark.dynamicAllocation.enabled":"true", -> set
"spark.dynamicAllocation.shuffleTracking.enabled":"true", -> set
"spark.dynamicAllocation.minExecutors":"1",
"spark.dynamicAllocation.maxExecutors":"10",
"spark.dynamicAllocation.initialExecutors":"1",
"spark.dynamicAllocation.schedulerBacklogTimeout": "1s",
"spark.dynamicAllocation.executorIdleTimeout": "5s"

Difference between spark driver and spark driver service
In Apache Spark, the driver is a core component responsible for orchestrating the execution of a Spark application. It manages the application’s lifecycle, schedules tasks, and maintains information about the application’s state. When deploying Spark on Kubernetes, the driver runs within a Kubernetes pod, often referred to as the driver pod.

To facilitate communication between the driver and executor pods, Spark creates a Kubernetes service associated with the driver pod. This driver service enables the executors to communicate with the driver, especially when the driver’s network address might change due to pod rescheduling or other factors.

Key Differences:
	•	Spark Driver:
	•	The main process that coordinates the execution of a Spark application.
	•	Schedules tasks and manages the application’s state.
	•	Runs as a pod in Kubernetes when deploying Spark in a Kubernetes environment.
	•	Spark Driver Service:
	•	A Kubernetes service created to provide a stable network endpoint for the driver pod.
	•	Ensures that executor pods can reliably communicate with the driver, even if the driver’s pod IP changes.
	•	Typically has a name following the pattern spark-<application-id>-driver-svc.

In summary, while the Spark driver is the process that manages the execution of a Spark application, the Spark driver service is a Kubernetes construct that ensures stable and reliable communication between the driver and executors within a Kubernetes cluster.

The Spark driver service is a concept specific to deploying Apache Spark on Kubernetes. In this environment, a Kubernetes service is created to provide a stable network endpoint for the Spark driver pod, facilitating reliable communication between the driver and executor pods. This setup ensures that executors can consistently connect to the driver, even if the driver’s pod IP changes due to rescheduling or other factors.

In contrast, when running Spark on YARN (Yet Another Resource Negotiator), such as in Cloudera distributions, the architecture differs. In YARN’s cluster mode, the Spark driver operates within the YARN Application Master process, which manages the application’s execution. In client mode, the driver runs in the client process that submits the application. In both modes, YARN handles resource allocation and task scheduling, and there isn’t a separate driver service as in Kubernetes deployments. ￼

Therefore, the concept of a Spark driver service is specific to Spark deployments on Kubernetes and is not applicable to Spark running on YARN or Cloudera platforms.


my job config:
spark.driver.cores=2
spark.driver.memory=64g
spark.executor.cores=4
spark.executor.memory=32g
spark.dynamicAllocation.minExecutors=4
spark.dynamicAllocation.maxExecutors=16
spark.dynamicAllocation.executorIdleTimeout=900s


